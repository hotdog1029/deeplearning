{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "신경망 모델의 문제점과 해결방법.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/e6yBeFD5jU1TtGbv53OH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hotdog1029/deeplearning/blob/main/%EC%8B%A0%EA%B2%BD%EB%A7%9D_%EB%AA%A8%EB%8D%B8%EC%9D%98_%EB%AC%B8%EC%A0%9C%EC%A0%90%EA%B3%BC_%ED%95%B4%EA%B2%B0%EB%B0%A9%EB%B2%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjdnkivl9KnN"
      },
      "source": [
        "# 과적합\r\n",
        "모델에 training data를 과하게 학습을 시키면, 모델은 training data에 좋은 성능을 나타내고, 오차나 MSE가 줄어들 수 있겠지만 training data가 아닌 새로운 data에 대해서는 좋지 않는 성능을 나타내고, 오차자 MSE가 커지게 된다. 즉 training data에 무한한 신뢰를 하여 모델에 과하게 적합을 하면, 일반화의 성능이 떨어지게 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyWHFsWh9LUV"
      },
      "source": [
        "# Gradient Vanishing Problem\r\n",
        "과적합 외에도 기본적인 신경망이 지니고 있는 단점이다. Gradient Vanishing Problem은 기울기가 사라지는 현상을 말한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jasz1jsZ9Lbs"
      },
      "source": [
        "딥러닝은 2개 이상의 Hidden Layer를 지니고 있는 다층 신경망이라 할 수 있다. 이러한 딥러닝이 발전하게 된 데에는 크게 두가지 원인이 있다. 첫째, 신경망의 단점으로 지적돼왔던 과적합과 Gradient Vanishing 문제를 완화시킬 수 있는 알고리즘이 발전한 것과 GPU를 신경망의 연산에 사용할 수 있게 되면서 CPU를 사용했을 때보다 적게는 10배, 많게는 수십 배 이상 빠른 속도로 딥러닝 연산을 수행할 수 있게 되면서 딥러닝이 발전하게 되었다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZTiACVhbubW"
      },
      "source": [
        "# MSE\r\n",
        "MSE(Mean Squared Error)는 기본적으로 회귀 모형에서 가장 많이 사용하는 Loss이자 성능 지표입니다. 신경망 모형뿐 아니라 선형 회귀 모델 또한 MSE를 Loss로서 사용합니다. 예측 값과 실제 값의 차이에 대해 평균 제곱합의 개념으로서 낮을수록 좋은 성능 지표이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqPxb9eq_4TG"
      },
      "source": [
        "# Dropout\r\n",
        "Dropout은 신경망의 학습 과정 중 Layer의 노드를 랜덤하게 Drop함으로써 Generalization 효과를 가져오게하는 테크닉이다. 즉, 이전 Epoch와 다른 Column에 0을 집어넣는 것이다. 이러한 방식으로 계속 연산하면 과적합을 어느 정도 방지하는 효과를 가져온다는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiXzh2jFdsEq"
      },
      "source": [
        "# ReLU 함수\r\n",
        "ReLU 함수는 기존의 시그모이드 함수와 같은 비선형 활성 함수가 지니고 있는 문제점을 어느 정도 해결한 함수이다. 활성 함수 ReLU는 입력 값이 0 이상이면 이 값을 그대로 출력하고 0 이하이면 0으로 출력하는 함수이다. 이 활성 함수가 시그모이드 함수에 비해 좋은 이유는 이 활성 함수를 미분할 때 입력 값이 0 이상인 부분은 기울기가 1, 입력 값이 0 이하인 부분은 0이 되기 때문이다. 즉, Back Propagation 과정 중 곱해지는 Activation 미분 값이 0 또는 1이 되기 때문에 아예 없애거나 완전히 살리는 것으로 해석할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxjm7ctjecf6"
      },
      "source": [
        "# Bacth Normalization\r\n",
        "신경망에는 과적합과 Gradient Vanishing 외에도 Internal Covariance shift 라는 현상이 발생한다. Internal Covariance shift란, 각 Layer마다 input 분포가 달라짐에 따라 학습 속도가 느려지는 현상을 말한다. Batch Normalization은 이를 방지하기 위한 기법으로, 말 그대로 Layer의 input 분포를 정규화해 학습 속도를 빠르게 하겠다는 것 이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awHBF8i-flvf"
      },
      "source": [
        "# Optimizer\r\n",
        "Batch 단위로 Back Propagation하는 과정을 SGD라 하고 이러한 과정을 Optimization 이라 한다. SGD 외에도 SGD의 단점을 보완하기 위한 다양한 Optimizer가 있다.\r\n",
        "\r\n",
        "Adaptive Moment Estimation(Adam): Adam은 딥러닝 모델을 디자인할 때 가장 기본적으로 가장 많이 사용하는 Optimizer로, RMSProp와 Momentum방식의 특징을 결합한 방법이다. 2020년 기준으로 많은 딥러닝 모델에서 기본적으로 Adam을 많이 사용하고 있다."
      ]
    }
  ]
}