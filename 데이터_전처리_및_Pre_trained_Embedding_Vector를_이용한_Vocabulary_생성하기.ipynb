{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "데이터 전처리 및 Pre-trained Embedding Vector를 이용한 Vocabulary 생성하기.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZeYZRrKp5qaZl7uT1TMy6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hotdog1029/deeplearning/blob/main/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC_%EB%B0%8F_Pre_trained_Embedding_Vector%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_Vocabulary_%EC%83%9D%EC%84%B1%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ofpg59vJbhC",
        "outputId": "3e461d9f-c503-4192-8892-e50df78cc414"
      },
      "source": [
        "import torch\r\n",
        "from torchtext import data\r\n",
        "from torchtext import datasets\r\n",
        "\r\n",
        "# Data Setting\r\n",
        "TEXT = data.Field(batch_first = True,\r\n",
        "                  fix_length = 500, # Sentence의 길이를 미리 제한하는 옵션\r\n",
        "                  tokenize = str.split, # Tokenize를 설정하는 옵션 기본값은 띄어쓰기 기반의 파이썬의 string.split 함수\r\n",
        "                  pad_first = True, # fix_length 대비 짧은 문장인 경의 Padding을 해야 하는데 Padding을 앞어세 중 것인지에 대한 옵션\r\n",
        "                  pad_token = '[PAD]', # 위에서 설명한 Padding에 대한 특수 Token 설정\r\n",
        "                  unk_token = '[UNK]') # Token Dictionary에 없는 Token이 나왔을 경우 해당 Token을 표현하는 특수 Token\r\n",
        "LABEL = data.LabelField(dtype = torch.float)\r\n",
        "\r\n",
        "train_data, test_data = datasets.IMDB.splits(text_field = TEXT,\r\n",
        "                                             label_field = LABEL)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:09<00:00, 8.42MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0hYrlIZMiXV",
        "outputId": "17fb8ffc-2460-4d0f-abb9-9fda38e65dbf"
      },
      "source": [
        "# Data Length\r\n",
        "print(f'Train Data Length: {len(train_data.examples)}')\r\n",
        "print(f'Test Data Length: {len(test_data.examples)}')\r\n",
        "\r\n",
        "# Data Fields\r\n",
        "print(train_data.fields)\r\n",
        "\r\n",
        "# Data Sample\r\n",
        "print('---- Data Sample ----')\r\n",
        "print('Input: ')\r\n",
        "print(' '.join(vars(train_data.examples[1])['text']),'\\\\n')\r\n",
        "print('Label: ')\r\n",
        "print(vars(train_data.examples[1])['label'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Data Length: 25000\n",
            "Test Data Length: 25000\n",
            "{'text': <torchtext.data.field.Field object at 0x7f7fc4618240>, 'label': <torchtext.data.field.LabelField object at 0x7f7fc4618198>}\n",
            "---- Data Sample ----\n",
            "Input: \n",
            "I have to point out, before you read this review, that in no way, is this a statement against Iranian people ... if you really want to read something into it, than hopefully you see, that I'm against politicians in general ... but if you're looking to be offended ... I can't help you!<br /><br />Not in Iran as this movie is banned there (see IMDb trivia for this movie). Which is a shame, because the movie is great. Would it not be for \"Grbavica\", this movie would have won at the International Film Festival in Berlin.<br /><br />Rightfully so (it was the runner-up, or second place if you will). Why? Because it is a movie about oppression. It's not even that this is a complete women issue. It is about the government trying to keep the people down. An analogy so clear that the government felt the need to ban the movie. But by banning it nothing is resolved and/or can they make this movie disappear! <br /><br />Another reviewer had a great summary line: \"Comedy about a tragedy\", that sums it up pretty well! \\n\n",
            "Label: \n",
            "pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmPdySXHpJRK"
      },
      "source": [
        "data.examples를 이용하면 데이터의 개수를 확인할 수 있고 vars() 함수를 이용하면 데이터 값을 직접 확인해볼 수 있다. Text Data는 Tokenize를 하기 전에 Data를 살펴보고 Cleansing 작업을 해야 한다. 다음과 같은 간단한 Data Cleansing 작업을 진행하였다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPDz9ZSdpbMb",
        "outputId": "1f264b01-34ab-4197-d7d9-556de6656abc"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "def PreProcessingText(input_sentence):\r\n",
        "  input_sentence = input_sentence.lower() # 소문자화\r\n",
        "  input_sentence = re.sub('<[^>]', repl = ' ', string = input_sentence) # '<br />' 처리\r\n",
        "  input_sentence = re.sub('[!\"#$%&\\\\()*+,-./:;<=>?@[\\\\\\\\]^_{|}~]', repl = ' ', string = input_sentence) # 특수 문자 처리\r\n",
        "  input_sentence = re.sub('\\\\s+', repl = ' ', string = input_sentence) # 연속된 띄어쓰기 처리\r\n",
        "  if input_sentence:\r\n",
        "    return input_sentence\r\n",
        "\r\n",
        "for example in train_data.examples:\r\n",
        "  vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()\r\n",
        "for example in test_data.examples:\r\n",
        "  vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()\r\n",
        "\r\n",
        "# pre-trained\r\n",
        "TEXT.build_vocab(train_data,\r\n",
        "                 min_freq = 2, # Vocab에 해당하는 Token에 최소한으로 등장하는 횟수를 제한을 둘 수 있다.\r\n",
        "                 max_size = None, # 위와 같이 Token의 최소 등장 횟수로 Vocab의 Size 조절을 하는 것 외에 전체 Vocab Size 자체에도 제한을 둘 수 있다.\r\n",
        "                 vectors = 'glove.6B.300d') # Pre-Trained Vector를 가져와 Vocab에 세팅하는 옵션, 원하는 Embedding을 정해 string 형태로 설정하면 된다.\r\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                           \n",
            "100%|█████████▉| 399773/400000 [00:44<00:00, 8865.10it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjuyz3ThttoI",
        "outputId": "a0cb9780-166a-47bc-ab70-529d3eca6793"
      },
      "source": [
        "# Vocabulary Info\r\n",
        "print(f'Vocab Size: {len(TEXT.vocab)}')\r\n",
        "\r\n",
        "print('Vocab Examples: ')\r\n",
        "for idx,(k,v) in enumerate(TEXT.vocab.stoi.items()):\r\n",
        "  if idx >= 10:\r\n",
        "    break\r\n",
        "  print('\\t', k ,v)\r\n",
        "print('-------------------')\r\n",
        "# Label Info\r\n",
        "print(f'Lable Size: {len(LABEL.vocab)}')\r\n",
        "\r\n",
        "print('Label Example: ')\r\n",
        "for idx,(k,v) in enumerate(LABEL.vocab.stoi.items()):\r\n",
        "  print('\\t',k,v)\r\n",
        "\r\n",
        "# Check embedding vectors\r\n",
        "print(TEXT.vocab.vectors.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size: 97856\n",
            "Vocab Examples: \n",
            "\t [UNK] 0\n",
            "\t [PAD] 1\n",
            "\t the 2\n",
            "\t a 3\n",
            "\t and 4\n",
            "\t of 5\n",
            "\t to 6\n",
            "\t is 7\n",
            "\t r 8\n",
            "\t in 9\n",
            "-------------------\n",
            "Lable Size: 2\n",
            "Label Example: \n",
            "\t neg 0\n",
            "\t pos 1\n",
            "torch.Size([97856, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0oHqVWDvBtN"
      },
      "source": [
        "이제 Valieation 구분과 Iterator를 이용해 Batch Data를 만들면 모델 학습을 위한 데이터 설정은 모두 끝난다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwLF2ypdvN2A"
      },
      "source": [
        "import random\r\n",
        "\r\n",
        "# Spliting Valid set\r\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(0),\r\n",
        "                                          split_ratio = 0.8)\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(datasets=(train_data, valid_data, test_data),\r\n",
        "                                                                           batch_size = 30,\r\n",
        "                                                                           device = device) "
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}