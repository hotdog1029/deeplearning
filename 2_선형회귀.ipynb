{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. 선형회귀.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqJSgIGM7NidhentZNtDJu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hotdog1029/deeplearning/blob/main/2_%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6MAWCJTqeDQ"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBxq3mimql7U",
        "outputId": "5c0c6933-6661-417b-b435-23b108a7de9d"
      },
      "source": [
        "# 데이터 설정\r\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\r\n",
        "y_train = torch.FloatTensor([[2],[4],[6]])\r\n",
        "\r\n",
        "# 모델 초기화\r\n",
        "W = torch.zeros(1, requires_grad=True) # 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시\r\n",
        "b = torch.zeros(1, requires_grad=True) # 편향 b를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시\r\n",
        "\r\n",
        "# optimizer 설정\r\n",
        "optimizer = optim.SGD([W,b], lr = 0.01) # SGD는 경사 하강법의 일종이다. lr(lerning rate)는 학습률을 의미한다. 학습 대상인 W와 b가 SGD의 입력이 된다.\r\n",
        "\r\n",
        "nb_epochs = 2000 # 원하는만큼 경사 하강법을 반복\r\n",
        "for epoch in range(nb_epochs+1):\r\n",
        "  # H(x) 계산\r\n",
        "  hypothesis = x_train * W + b # 가설 세우기\r\n",
        "\r\n",
        "  # cost(loss) 계산\r\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2) # 선형회귀의 비용함수에 해당되는 평균 제곱 오차를 선언\r\n",
        "\r\n",
        "  # cost로 H(x) 개선\r\n",
        "  optimizer.zero_grad() # gradient를 0으로 초기화\r\n",
        "  cost.backward() # 비용함수를 미분하여 gradient 계산\r\n",
        "  optimizer.step() # W와 b를 업데이트\r\n",
        "\r\n",
        "  # 100번마다 출력\r\n",
        "  if epoch % 100 == 0:\r\n",
        "    print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs,W.item(),b.item(),cost.item()))\r\n",
        "\r\n",
        "# 선형회귀는 결국 원하는만큼 경사하강법을 반복하여 cost(loss)를 줄여나가 W(가중치)와 b(편향)을 구하는 것이다.\r\n",
        "#  optimizer.zero_grad()는 미분값이 누적되는것을 방지하여 0으로 계속 초기화 시켜주기위함이다.\r\n",
        "# requires_grad=True, backward() 등은 파이토치에서 제공하고 있는 자동 미분 기능을 수행하는 것이다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/2000 W: 0.187, b: 0.080 Cost: 18.666666\n",
            "Epoch  100/2000 W: 1.746, b: 0.578 Cost: 0.048171\n",
            "Epoch  200/2000 W: 1.800, b: 0.454 Cost: 0.029767\n",
            "Epoch  300/2000 W: 1.843, b: 0.357 Cost: 0.018394\n",
            "Epoch  400/2000 W: 1.876, b: 0.281 Cost: 0.011366\n",
            "Epoch  500/2000 W: 1.903, b: 0.221 Cost: 0.007024\n",
            "Epoch  600/2000 W: 1.924, b: 0.174 Cost: 0.004340\n",
            "Epoch  700/2000 W: 1.940, b: 0.136 Cost: 0.002682\n",
            "Epoch  800/2000 W: 1.953, b: 0.107 Cost: 0.001657\n",
            "Epoch  900/2000 W: 1.963, b: 0.084 Cost: 0.001024\n",
            "Epoch 1000/2000 W: 1.971, b: 0.066 Cost: 0.000633\n",
            "Epoch 1100/2000 W: 1.977, b: 0.052 Cost: 0.000391\n",
            "Epoch 1200/2000 W: 1.982, b: 0.041 Cost: 0.000242\n",
            "Epoch 1300/2000 W: 1.986, b: 0.032 Cost: 0.000149\n",
            "Epoch 1400/2000 W: 1.989, b: 0.025 Cost: 0.000092\n",
            "Epoch 1500/2000 W: 1.991, b: 0.020 Cost: 0.000057\n",
            "Epoch 1600/2000 W: 1.993, b: 0.016 Cost: 0.000035\n",
            "Epoch 1700/2000 W: 1.995, b: 0.012 Cost: 0.000022\n",
            "Epoch 1800/2000 W: 1.996, b: 0.010 Cost: 0.000013\n",
            "Epoch 1900/2000 W: 1.997, b: 0.008 Cost: 0.000008\n",
            "Epoch 2000/2000 W: 1.997, b: 0.006 Cost: 0.000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kYOgAQavrfo",
        "outputId": "d6a75161-c47e-46c4-d1d8-3f384703d923"
      },
      "source": [
        "# 다중 선형 회귀(다수의 x로부터 y를 예측)\r\n",
        "\r\n",
        "# 데이터 설정\r\n",
        "x1_train = torch.FloatTensor([[73],[93],[89],[96],[73]])\r\n",
        "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\r\n",
        "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\r\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\r\n",
        "\r\n",
        "# 가충치 w와 편향 b 초기화\r\n",
        "w1 = torch.zeros(1, requires_grad=True)\r\n",
        "w2 = torch.zeros(1, requires_grad=True)\r\n",
        "w3 = torch.zeros(1, requires_grad=True)\r\n",
        "b = torch.zeros(1, requires_grad=True)\r\n",
        "\r\n",
        "# optimizer 설정\r\n",
        "optimizer = optim.SGD([w1,w2,w3,b], lr = 1e-5)\r\n",
        "\r\n",
        "# 원하는 횟수만큼 경사하강법 반복\r\n",
        "nb_epochs = 5000\r\n",
        "for epoch in range(nb_epochs+1):\r\n",
        "  # H(x) 계산\r\n",
        "  hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\r\n",
        "\r\n",
        "  # cost 계산\r\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\r\n",
        "\r\n",
        "  # cost로 H(x) 개선\r\n",
        "  optimizer.zero_grad()\r\n",
        "  cost.backward()\r\n",
        "  optimizer.step()\r\n",
        "\r\n",
        "  # 100번 마다 출력\r\n",
        "  if epoch % 100 == 0:\r\n",
        "    print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\r\n",
        "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\r\n",
        "        ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/5000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
            "Epoch  100/5000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563628\n",
            "Epoch  200/5000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497595\n",
            "Epoch  300/5000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435044\n",
            "Epoch  400/5000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375726\n",
            "Epoch  500/5000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319507\n",
            "Epoch  600/5000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
            "Epoch  700/5000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215703\n",
            "Epoch  800/5000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167810\n",
            "Epoch  900/5000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
            "Epoch 1000/5000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079390\n",
            "Epoch 1100/5000 w1: 0.722 w2: 0.608 w3: 0.680 b: 0.009 Cost: 1.038574\n",
            "Epoch 1200/5000 w1: 0.727 w2: 0.603 w3: 0.681 b: 0.010 Cost: 0.999884\n",
            "Epoch 1300/5000 w1: 0.731 w2: 0.599 w3: 0.681 b: 0.010 Cost: 0.963217\n",
            "Epoch 1400/5000 w1: 0.735 w2: 0.595 w3: 0.681 b: 0.010 Cost: 0.928427\n",
            "Epoch 1500/5000 w1: 0.739 w2: 0.591 w3: 0.681 b: 0.010 Cost: 0.895448\n",
            "Epoch 1600/5000 w1: 0.743 w2: 0.586 w3: 0.682 b: 0.010 Cost: 0.864169\n",
            "Epoch 1700/5000 w1: 0.746 w2: 0.583 w3: 0.682 b: 0.010 Cost: 0.834509\n",
            "Epoch 1800/5000 w1: 0.750 w2: 0.579 w3: 0.682 b: 0.010 Cost: 0.806380\n",
            "Epoch 1900/5000 w1: 0.754 w2: 0.575 w3: 0.682 b: 0.010 Cost: 0.779696\n",
            "Epoch 2000/5000 w1: 0.757 w2: 0.571 w3: 0.682 b: 0.011 Cost: 0.754379\n",
            "Epoch 2100/5000 w1: 0.760 w2: 0.568 w3: 0.682 b: 0.011 Cost: 0.730373\n",
            "Epoch 2200/5000 w1: 0.764 w2: 0.564 w3: 0.682 b: 0.011 Cost: 0.707601\n",
            "Epoch 2300/5000 w1: 0.767 w2: 0.561 w3: 0.682 b: 0.011 Cost: 0.685991\n",
            "Epoch 2400/5000 w1: 0.770 w2: 0.558 w3: 0.682 b: 0.011 Cost: 0.665485\n",
            "Epoch 2500/5000 w1: 0.773 w2: 0.555 w3: 0.682 b: 0.011 Cost: 0.646028\n",
            "Epoch 2600/5000 w1: 0.776 w2: 0.552 w3: 0.682 b: 0.011 Cost: 0.627574\n",
            "Epoch 2700/5000 w1: 0.779 w2: 0.549 w3: 0.682 b: 0.012 Cost: 0.610050\n",
            "Epoch 2800/5000 w1: 0.782 w2: 0.546 w3: 0.682 b: 0.012 Cost: 0.593426\n",
            "Epoch 2900/5000 w1: 0.785 w2: 0.543 w3: 0.682 b: 0.012 Cost: 0.577643\n",
            "Epoch 3000/5000 w1: 0.788 w2: 0.541 w3: 0.682 b: 0.012 Cost: 0.562653\n",
            "Epoch 3100/5000 w1: 0.791 w2: 0.538 w3: 0.682 b: 0.012 Cost: 0.548430\n",
            "Epoch 3200/5000 w1: 0.793 w2: 0.535 w3: 0.682 b: 0.012 Cost: 0.534914\n",
            "Epoch 3300/5000 w1: 0.796 w2: 0.533 w3: 0.682 b: 0.012 Cost: 0.522085\n",
            "Epoch 3400/5000 w1: 0.798 w2: 0.530 w3: 0.682 b: 0.012 Cost: 0.509902\n",
            "Epoch 3500/5000 w1: 0.801 w2: 0.528 w3: 0.682 b: 0.012 Cost: 0.498333\n",
            "Epoch 3600/5000 w1: 0.803 w2: 0.526 w3: 0.681 b: 0.013 Cost: 0.487337\n",
            "Epoch 3700/5000 w1: 0.806 w2: 0.524 w3: 0.681 b: 0.013 Cost: 0.476889\n",
            "Epoch 3800/5000 w1: 0.808 w2: 0.522 w3: 0.681 b: 0.013 Cost: 0.466959\n",
            "Epoch 3900/5000 w1: 0.810 w2: 0.519 w3: 0.681 b: 0.013 Cost: 0.457530\n",
            "Epoch 4000/5000 w1: 0.812 w2: 0.517 w3: 0.681 b: 0.013 Cost: 0.448557\n",
            "Epoch 4100/5000 w1: 0.814 w2: 0.515 w3: 0.681 b: 0.013 Cost: 0.440033\n",
            "Epoch 4200/5000 w1: 0.817 w2: 0.514 w3: 0.680 b: 0.013 Cost: 0.431924\n",
            "Epoch 4300/5000 w1: 0.819 w2: 0.512 w3: 0.680 b: 0.013 Cost: 0.424211\n",
            "Epoch 4400/5000 w1: 0.821 w2: 0.510 w3: 0.680 b: 0.014 Cost: 0.416875\n",
            "Epoch 4500/5000 w1: 0.823 w2: 0.508 w3: 0.680 b: 0.014 Cost: 0.409904\n",
            "Epoch 4600/5000 w1: 0.825 w2: 0.507 w3: 0.679 b: 0.014 Cost: 0.403257\n",
            "Epoch 4700/5000 w1: 0.826 w2: 0.505 w3: 0.679 b: 0.014 Cost: 0.396936\n",
            "Epoch 4800/5000 w1: 0.828 w2: 0.503 w3: 0.679 b: 0.014 Cost: 0.390922\n",
            "Epoch 4900/5000 w1: 0.830 w2: 0.502 w3: 0.679 b: 0.014 Cost: 0.385190\n",
            "Epoch 5000/5000 w1: 0.832 w2: 0.500 w3: 0.678 b: 0.014 Cost: 0.379739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s88hX2I3ql-R",
        "outputId": "f2747a9e-6106-4e3f-c860-47af2b8b17d5"
      },
      "source": [
        "# nn.Module로 구현하는 선형 회귀\r\n",
        "\r\n",
        "# 데이터\r\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\r\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])\r\n",
        "# 모델 선언 및 초기화, 단순 선형 회귀이므로 input_dim = 1 ,output_dim = 1\r\n",
        "model = nn.Linear(1,1) # model에는 가중치 W와 편향 b가 저장되어 있다.\r\n",
        "\r\n",
        "# optimizer 설정 \r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01) # 경사 하강법 SGD를 사용하고 lr(learning rate)은 0.01로 설정\r\n",
        "\r\n",
        "# 경사하강법 2000번 반복\r\n",
        "nb_epochs = 2000\r\n",
        "for epoch in range(nb_epochs +1):\r\n",
        "\r\n",
        "  # H(x) 계산\r\n",
        "  prediction = model(x_train)\r\n",
        "\r\n",
        "  # cost 계산\r\n",
        "  cost = F.mse_loss(prediction, y_train) # 파이토치에서 제공하는 평균 제곱오차함수\r\n",
        "\r\n",
        "  optimizer.zero_grad()\r\n",
        "  cost.backward()\r\n",
        "  optimizer.step()\r\n",
        "\r\n",
        "  if epoch % 100 == 0:\r\n",
        "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\r\n",
        "          epoch, nb_epochs, cost.item()\r\n",
        "      ))\r\n",
        "print(list(model.parameters())) # W값이 2에 가깝고 b의 값이 0에 가까운것을 볼 수 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/2000 Cost: 26.487640\n",
            "Epoch  100/2000 Cost: 0.148082\n",
            "Epoch  200/2000 Cost: 0.091506\n",
            "Epoch  300/2000 Cost: 0.056545\n",
            "Epoch  400/2000 Cost: 0.034941\n",
            "Epoch  500/2000 Cost: 0.021592\n",
            "Epoch  600/2000 Cost: 0.013342\n",
            "Epoch  700/2000 Cost: 0.008245\n",
            "Epoch  800/2000 Cost: 0.005095\n",
            "Epoch  900/2000 Cost: 0.003148\n",
            "Epoch 1000/2000 Cost: 0.001945\n",
            "Epoch 1100/2000 Cost: 0.001202\n",
            "Epoch 1200/2000 Cost: 0.000743\n",
            "Epoch 1300/2000 Cost: 0.000459\n",
            "Epoch 1400/2000 Cost: 0.000284\n",
            "Epoch 1500/2000 Cost: 0.000175\n",
            "Epoch 1600/2000 Cost: 0.000108\n",
            "Epoch 1700/2000 Cost: 0.000067\n",
            "Epoch 1800/2000 Cost: 0.000041\n",
            "Epoch 1900/2000 Cost: 0.000026\n",
            "Epoch 2000/2000 Cost: 0.000016\n",
            "[Parameter containing:\n",
            "tensor([[1.9954]], requires_grad=True), Parameter containing:\n",
            "tensor([0.0105], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5rdjaBEqmBh",
        "outputId": "7cbed966-c04b-46fe-a194-40f3a06d3aef"
      },
      "source": [
        "# nn.Module로 구현하는 다중 회귀\r\n",
        "\r\n",
        "# 데이터\r\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\r\n",
        "                             [93, 88, 93],\r\n",
        "                             [89, 91, 90],\r\n",
        "                             [96, 98, 100],\r\n",
        "                             [73, 66, 70]])\r\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\r\n",
        "\r\n",
        "# 모델을 선언 및 초기화. 다중 선형 회귀이므로 input_dim=3, output_dim=1.\r\n",
        "model = nn.Linear(3,1)\r\n",
        "\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\r\n",
        "\r\n",
        "nb_epochs = 2000\r\n",
        "for epoch in range(nb_epochs+1):\r\n",
        "\r\n",
        "    # H(x) 계산\r\n",
        "    prediction = model(x_train)\r\n",
        "    # model(x_train)은 model.forward(x_train)와 동일함.\r\n",
        "\r\n",
        "    # cost 계산\r\n",
        "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\r\n",
        "\r\n",
        "    # cost로 H(x) 개선하는 부분\r\n",
        "    # gradient를 0으로 초기화\r\n",
        "    optimizer.zero_grad()\r\n",
        "    # 비용 함수를 미분하여 gradient 계산\r\n",
        "    cost.backward()\r\n",
        "    # W와 b를 업데이트\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    if epoch % 100 == 0:\r\n",
        "    # 100번마다 로그 출력\r\n",
        "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\r\n",
        "          epoch, nb_epochs, cost.item()\r\n",
        "      ))\r\n",
        "\r\n",
        "\r\n",
        "print(list(model.parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/2000 Cost: 9229.115234\n",
            "Epoch  100/2000 Cost: 1.889843\n",
            "Epoch  200/2000 Cost: 1.805329\n",
            "Epoch  300/2000 Cost: 1.725269\n",
            "Epoch  400/2000 Cost: 1.649375\n",
            "Epoch  500/2000 Cost: 1.577483\n",
            "Epoch  600/2000 Cost: 1.509351\n",
            "Epoch  700/2000 Cost: 1.444783\n",
            "Epoch  800/2000 Cost: 1.383609\n",
            "Epoch  900/2000 Cost: 1.325598\n",
            "Epoch 1000/2000 Cost: 1.270651\n",
            "Epoch 1100/2000 Cost: 1.218589\n",
            "Epoch 1200/2000 Cost: 1.169212\n",
            "Epoch 1300/2000 Cost: 1.122436\n",
            "Epoch 1400/2000 Cost: 1.078090\n",
            "Epoch 1500/2000 Cost: 1.036060\n",
            "Epoch 1600/2000 Cost: 0.996226\n",
            "Epoch 1700/2000 Cost: 0.958475\n",
            "Epoch 1800/2000 Cost: 0.922675\n",
            "Epoch 1900/2000 Cost: 0.888736\n",
            "Epoch 2000/2000 Cost: 0.856582\n",
            "[Parameter containing:\n",
            "tensor([[0.7966, 0.6159, 0.6044]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.4597], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeVG8SplqmEF",
        "outputId": "5a0a9502-de63-4bf8-f014-2ca27547b966"
      },
      "source": [
        "# 클래스로 구현\r\n",
        "\r\n",
        "# 데이터\r\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\r\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])\r\n",
        "\r\n",
        "# 클래스를 구현\r\n",
        "class LinearRegressionModel(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.linear = nn.Linear(1, 1)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self.linear(x)\r\n",
        "\r\n",
        "model = LinearRegressionModel()\r\n",
        "\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\r\n",
        "\r\n",
        "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\r\n",
        "nb_epochs = 2000\r\n",
        "for epoch in range(nb_epochs+1):\r\n",
        "\r\n",
        "    # H(x) 계산\r\n",
        "    prediction = model(x_train)\r\n",
        "\r\n",
        "    # cost 계산\r\n",
        "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\r\n",
        "\r\n",
        "    # cost로 H(x) 개선하는 부분\r\n",
        "    # gradient를 0으로 초기화\r\n",
        "    optimizer.zero_grad()\r\n",
        "    # 비용 함수를 미분하여 gradient 계산\r\n",
        "    cost.backward() # backward 연산\r\n",
        "    # W와 b를 업데이트\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    if epoch % 100 == 0:\r\n",
        "    # 100번마다 로그 출력\r\n",
        "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\r\n",
        "          epoch, nb_epochs, cost.item()\r\n",
        "      ))\r\n",
        "\r\n",
        "print(list(model.parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/2000 Cost: 14.315708\n",
            "Epoch  100/2000 Cost: 0.000092\n",
            "Epoch  200/2000 Cost: 0.000057\n",
            "Epoch  300/2000 Cost: 0.000035\n",
            "Epoch  400/2000 Cost: 0.000022\n",
            "Epoch  500/2000 Cost: 0.000013\n",
            "Epoch  600/2000 Cost: 0.000008\n",
            "Epoch  700/2000 Cost: 0.000005\n",
            "Epoch  800/2000 Cost: 0.000003\n",
            "Epoch  900/2000 Cost: 0.000002\n",
            "Epoch 1000/2000 Cost: 0.000001\n",
            "Epoch 1100/2000 Cost: 0.000001\n",
            "Epoch 1200/2000 Cost: 0.000000\n",
            "Epoch 1300/2000 Cost: 0.000000\n",
            "Epoch 1400/2000 Cost: 0.000000\n",
            "Epoch 1500/2000 Cost: 0.000000\n",
            "Epoch 1600/2000 Cost: 0.000000\n",
            "Epoch 1700/2000 Cost: 0.000000\n",
            "Epoch 1800/2000 Cost: 0.000000\n",
            "Epoch 1900/2000 Cost: 0.000000\n",
            "Epoch 2000/2000 Cost: 0.000000\n",
            "[Parameter containing:\n",
            "tensor([[2.0001]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.0003], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}