{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5. 퍼셉트론.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOT9jCQE7/rOPupaeeg+9YO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hotdog1029/deeplearning/blob/main/5_%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LeC4tUKgmEd"
      },
      "source": [
        "# 퍼셉트론\r\n",
        "퍼셉트론(perceptron)은 1957년에 개발된 최초의 인공지능 모형이다. Feed-Forward Network 모형의 가장 간단한 형태이며 선형 분류 모형의 형태를 띠고있다. 이 선형 결합의 값에 특정 임계값의 초과 여부를 판단하는 함수를 적용한다. 이 출력 값이 0보다 크면 1, 작으면 -1로 결과값을 분류하는 모형을 '퍼셉트론'이라고 한다.\r\n",
        "여기서 임계값의 초과 여부를 판단하는 함수를 '활성화 함수(Activation Function)'이라고 한다. 가장 기본적인 Activation 함수른 Step Function으로 input 값이 0 이상이면 1, 이상이 아니면 -1을 출력하는 함수이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXM_m1GBgmKA"
      },
      "source": [
        "# MLP(Multi Layer Perceptron)\r\n",
        "퍼셉트론이 비선형 분류 문제는 풀지 못한다는 단점이 있다. 이러한 퍼셉트론이 지니고 있는 한계점을 극복하기 위해 여러 Layer를 쌓아올린 MLP가 등장하게 되었다. 퍼셉트론은 input과 Output layer만으로 구성돼 있지만, MLP는 중건에 hidden layer를 넣은 형태이다. 즉, MLP는 여러 개의 퍼셉트론 조합과 이것들의 재조합으로 복잡한 비선형적인 모형을 만들어내는 것이라 할 수 있다. 딥러닝의 기본 구조가 되는 것이 신경망이라면 기본적으로 MLP를 말한다. MLP의 Hidden Layer를 쌓으면 Layer가 깊어 지기 때문에 딥러닝의 기본적이 모델이 된다. input 1개, hidden 1개, output 1개 즉 총 3개의 layer로 연결돼 있는 MLP이며 각 원은 노드라고 부른다. inuput node의 수는 input data의 변수의 수가 되고 hidden layer와 hidden node의 수는 사용자가 지정해야 할 하이퍼파라미터이다. output layer는 최종적으로 모델의 결과값을 의미하기 때문에 output node의 수는 풀고자 하는 문제에 따라 달라진다. 회귀 분석의 경우는 1개, 0부터 9까지의 숫자 분류를 하고자 하는 경우 output node의 수는 10이 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx5y6JtSgmQw"
      },
      "source": [
        "# Feed Forward\r\n",
        "신경망은 input에서 weight와 hidden을 거쳐 output을 내보낸다. 이 과정을 'Feed Forward'라 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZjD1Bw5n-k1"
      },
      "source": [
        "# Back Propagation\r\n",
        "Feed Forward를 이용해 input에서 output까지 계산한다. 여기서 output은 우리가 얻고자 하는 예측 값을 의미한다. 그리고 모델의 예측 값과 실제 값의 차이를 계산한다. 이 에러를 바탕으로 신경망의 weight를 업데이트한다. 뒤의 weight부터 업데이트하고 이후에 앞의 weight를 업데이트 한다. Feed Forward를 이용해 계산된 에러를 바탕으로 뒤의 weight부터 업데이트한다고 해서 이 과정을 'Back Propagation'이라고 한다. Feed Forward와 Back Propagation을 계속 반복하면서 weight를점차 신경망의 ou 업데이트하며 tput이 실제 값에 가까워 지면서 모델의 학습이 이루어진다. 이 반복하는 횟수를 'Epoch(세대)'라고 한다. 100Epoch라고 하면 전체 데이터셋에 대해 100번의 Feed Forward와 Back Propagation을 했다고 이해하면 됨. 이를 정리하면 신경망의 Input -> Hidden -> Output까지는 데이터에서 예측 값을 계산하는 Feed Forward 과정, Output에서 에러를 계산해 Weight를 업데이트하는 과정을 '역전파(Back Propagation)' 과정이라고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeNV3EF9rE2o"
      },
      "source": [
        "# 활성 함수\r\n",
        "활성 함수는 어떤 신호를 입력받아 이를 적절히 처리해 출력해주는 함수를 의미한다. 앞서 퍼셉트론에서 간단한 현태의 활성함수를 알아보았다. 신경망에서는 기본적으로 비선형 활성 함수를 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lvjCe77rFE_"
      },
      "source": [
        "# 시그모이드 함수\r\n",
        "비선형 활성 함수 중 가장 기본적으로 사용하는 함수는 '시그모이드 함수(sigmoid function)이다. 시그모이드 함수는 입력 값이 0이하이면 0.5 이하의 값을 출력하고 0 이상이면 0.5 이상의 값을 출력한다. 다시말해, 입력 값에 대해 0부터 1사이로 Scaling해주는 개념이라 보면 된다. 이 비선형 활성 함수를 사용하는 이유는 우리가 풀고자 하는 것이 비선형적인 복잡한 문제이기 때문이다. input과 weight의 선형 결합이 활성 함수로 들어가게 된다. 즉 선형결합을 비선형화한 개념이라 볼 수 있다. 기존에는 신경망 모형이 직선밖에 긋지 못했다면 비선형화시켜 여러 곡선의 조합으로 복잡한 문제를 풀 수 있게 된 것이다. 그러나 시그모이드 함수는 Back Propagation 과정 중 Gradient Vanishing 현상이 발생할 수 있다. 모델이 깊어질수록 심해짐"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2H-1KZ0s2ZD"
      },
      "source": [
        "# Gradient Descent Method(기울기 경사 하강법)\r\n",
        "가장 간단한 선형 회귀 모형은 Loss Function을 MSE로 설정해 MSE가 감소하도록 회귀계수를 추정한다. 그렇다면 이 MSE는 회귀계수에 대한 함수로 볼 수 있다. MSE는 2차 함수의 형태이므로 이 MSE가 최소가 되는 지점을 찾을 수 있다. MSE는 회귀계수에 대한 함수이므로 회귀계수로 미분해 기울기가 0이 되는 지점을 찾으면 그때의 회귀계수는 MSE가 최소로 만드는 회귀 계수이다. 그런데 선형 회귀 모델의 경우에는 MSE가 최소가 되는 지점을 한 번에 찾을 수 있다. 직선으로 이뤄진 비교적 간단한 모델이기 때문에 MSE를 회귀계수로 미분해 기울기가 0이 되는 지점을 한 번에 찾을 수 있다. 그러나 신경망 모형은 Hidden Layer가 깊어질수록 Hidden Layer가 많아질수록 더욱 복잡해진다. 그렇기 때문에 신경망 모형에서 MSE를 신경망 모형의 weight로 미분해 기울기가 0인 지점을 찾을 수 없다. 그래서 MSE를 신경망 모형의 weight로 미분해 기울기를 감소기켜 최소가 되는 지점을 찾아간다.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvVcuJKJvDt4"
      },
      "source": [
        "# Batch Size\r\n",
        "신경망의 학습은 Feed Forward와 Back Propagation을 번갈아가며 진행되며 학습에 따라 점차 학습 데이터에 대한 MSE가 줄어든다. 여기서 우리가 갖고 있는 모든 데이터를 한 번에 Feed Forward하지는 않는다. 너무 많은 연산을 필요로 하는 컴퓨팅 문제가 발생하기 때문에 매우 비효율적인 학습 과정을 거치게 된다. 그래서 우리가 갖고 있는 데이터를 쪼개 Feed Forward한다. 우리가 갖고 있는 전체 데이터가 1000개라 하면 100개씩 쪼개 Feed Forward와 Back Propagation을 10번 반복한다. 이 한 과정을 'Epoch'이라 하고 여기서 100개의 데이터를 'Mini-Batch'라 하며 100의 크기에 대해서는 'Batch Size'라고 한다. 이렇게 데이터를 쪼개 Gradient Desent Method 하는 방법을 'Stochastic Gradient Descent(SGD)'라고 부르며 이렇게 Gradient Descent 해주는 것을 통틀어 'Optimizer'이라고 한다. 일반적으로 구해지는 Gradient의 크기는 매우크다. 따라서 이 Gradient의 크기를 조절해줄 필요가 있는데, 이 조절해주는 상수를 Learning Rate라 부른다. Learning Rate를 지정해주지 않으면 Gradient를 구했을 때 Loss가 제대로 감소되는 방향을 구하지 못할 확률이 놓고 아예 학습되지 않는 경우도 많다. 그렇기 때문에 Learning Rate는 적절히 지정해줘야 하는데, 일반적으로 0.01 아래로 작은 값을 주면서 어러 번 학습을 시도해가며 조절한다."
      ]
    }
  ]
}